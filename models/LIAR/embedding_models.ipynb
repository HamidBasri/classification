{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core\n",
    "import re\n",
    "\n",
    "#Data Manipulation \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Data Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Preprocessing Tools\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "\n",
    "\n",
    "# NLP Tools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Data Evaluation \n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# Neural Networks\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_hub as hub\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "# Ignore any warnings\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "col_names = [\n",
    "        'id',               # Column 1: the ID of the statement ([ID].json).\n",
    "        'label',            # Column 2: the label.\n",
    "        'statement',        # Column 3: the statement.\n",
    "        'subjects',         # Column 4: the subject(s).\n",
    "        'speaker',          # Column 5: the speaker.\n",
    "        'speaker_job_title', # Column 6: the speaker's job title.\n",
    "        'state_info',       # Column 7: the state info.\n",
    "        'party_affiliation', # Column 8: the party affiliation.\n",
    "        \n",
    "        'barely_true', # barely true counts.\n",
    "        'false', # false counts.\n",
    "        'half_true', # half true counts.\n",
    "        'mostly_true', # mostly true counts.\n",
    "        'pants_on_fire', # pants on fire counts.\n",
    "        \n",
    "        'context' # Column 14: the context (venue / location of the speech or statement).\n",
    "    ]\n",
    "\n",
    "def read_df(tsv_file: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(tsv_file, delimiter='\\t', dtype=object)\n",
    "    # replaces all \"null\" or \"NaN\" values with an empty string\n",
    "    df.fillna(\"\", inplace=True)\n",
    "    # labels the columns in the dataset using the data dictionary described in the README\n",
    "    df.columns = col_names\n",
    "    df = df.dropna(subset=[\"label\", \"statement\"])\n",
    "    df.drop([\"id\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = read_df('datasets/train.tsv')\n",
    "test_df = read_df('datasets/test.tsv')\n",
    "valid_df = read_df('datasets/valid.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reset_index(df):\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def drop_na(df):\n",
    "    df = df.dropna(subset=['input'], axis=0)\n",
    "    return df\n",
    "\n",
    "def drop_duplicated(df):\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "def label(df):\n",
    "    df['output'] = df['label'].map({'true': 1,\n",
    "                                     'mostly-true': 1,\n",
    "                                     'half-true': 1,\n",
    "                                     'false': 0,\n",
    "                                     'barely-true': 0,\n",
    "                                     'pants-fire': 0}).astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    processed_text = text.lower()\n",
    "    \n",
    "    processed_text=re.sub(re.compile('[/(){}\\[\\]\\|@,;]'),' ',processed_text)\n",
    "    processed_text=re.sub(re.compile('[^0-9a-z #+_]'),' ',processed_text)\n",
    "    \n",
    "     # Tokenization\n",
    "    words = word_tokenize(processed_text)\n",
    "    \n",
    "    # Lemmatize and stem each word\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_words = [lemmatizer.lemmatize(stemmer.stem(word)) for word in words]\n",
    "    \n",
    "    # Stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['say', 'percent', 'state', 'year', 'said', 'people', 'one'])\n",
    "    filtered_words = [word for word in stemmed_words if word.lower() not in stop_words]\n",
    "    # Combine words back into a sentence\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def clean_df(df):\n",
    "    df['input'] = df['input'].apply(lambda x: clean_text(x))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_pipeline = Pipeline(steps=[\n",
    "    ('drop_na', FunctionTransformer(drop_na)),\n",
    "    ('drop_duplicated', FunctionTransformer(drop_duplicated)),\n",
    "    ('label', FunctionTransformer(label)),\n",
    "    # ('clean', FunctionTransformer(clean_df)),\n",
    "    ('reset_index', FunctionTransformer(reset_index))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = [\"statement\", \"subjects\", \"speaker\", \"context\", \"speaker_job_title\", \"state_info\", 'barely_true', 'false', 'half_true', 'mostly_true', 'pants_on_fire']\n",
    "# x_features = [\"statement\"]\n",
    "y_feature = [\"output\"]\n",
    "\n",
    "\n",
    "train_df[\"input\"] = train_df[x_features].apply(lambda row: ' '.join(map(str, row)), axis=1)\n",
    "test_df[\"input\"] = test_df[x_features].apply(lambda row: ' '.join(map(str, row)), axis=1)\n",
    "valid_df[\"input\"] = valid_df[x_features].apply(lambda row: ' '.join(map(str, row)), axis=1)\n",
    "\n",
    "# Clean all datasets\n",
    "train_df = cleaning_pipeline.fit_transform(train_df)\n",
    "test_df = cleaning_pipeline.transform(test_df)\n",
    "valid_df = cleaning_pipeline.transform(valid_df)\n",
    "\n",
    "\n",
    "train_clean_df = train_df[[\"input\", \"output\"]]\n",
    "test_clean_df = test_df[[\"input\", \"output\"]]\n",
    "valid_clean_df = valid_df[[\"input\", \"output\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>declin coal start start natur ga took start begin presid georg w bush administr energi histori job accomplish scott surovel floor speech deleg virginia 0 0 1 1 0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hillari clinton agre john mccain vote give georg bush benefit doubt iran foreign polici barack obama denver presid illinoi 70 71 160 163 9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>health care reform legisl like mandat free sex chang surgeri health care blog post news releas 7 19 3 5 44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>econom turnaround start end term economi job charli crist interview cnn florida 15 9 20 19 2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chicago bear start quarterback last 10 total number tenur uw faculti fire dure last two decad educ robin vo onlin opinion piec wisconsin assembl speaker wisconsin 0 3 2 5 1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                          input  \\\n",
       "0             declin coal start start natur ga took start begin presid georg w bush administr energi histori job accomplish scott surovel floor speech deleg virginia 0 0 1 1 0   \n",
       "1                                    hillari clinton agre john mccain vote give georg bush benefit doubt iran foreign polici barack obama denver presid illinoi 70 71 160 163 9   \n",
       "2                                                                    health care reform legisl like mandat free sex chang surgeri health care blog post news releas 7 19 3 5 44   \n",
       "3                                                                                  econom turnaround start end term economi job charli crist interview cnn florida 15 9 20 19 2   \n",
       "4  chicago bear start quarterback last 10 total number tenur uw faculti fire dure last two decad educ robin vo onlin opinion piec wisconsin assembl speaker wisconsin 0 3 2 5 1   \n",
       "\n",
       "   output  \n",
       "0       1  \n",
       "1       1  \n",
       "2       0  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "train_clean_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode y_values\n",
    "le = LabelEncoder()\n",
    "\n",
    "train_labels = le.fit_transform(train_clean_df[\"output\"])\n",
    "val_labels = le.transform(valid_clean_df[\"output\"])\n",
    "test_labels = le.transform(test_clean_df[\"output\"])\n",
    "\n",
    "train_labels = np.asarray(to_categorical(train_labels))\n",
    "val_labels = np.asarray(to_categorical(val_labels))\n",
    "test_labels = np.asarray(to_categorical(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-29 04:00:34.342716: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-01-29 04:00:34.342745: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-01-29 04:00:34.342752: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-01-29 04:00:34.342962: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-29 04:00:34.342989: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-01-29 04:00:37.700551: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sentences to embeddings\n",
    "X_train = embed(train_clean_df[\"input\"])\n",
    "X_val = embed(valid_clean_df[\"input\"])\n",
    "X_test = embed(test_clean_df[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Sequential()\n",
    "final_model.add(Dense(units=128, activation=\"relu\"))\n",
    "final_model.add(Dropout(0.3))\n",
    "final_model.add(Dense(units=64, activation=\"relu\"))\n",
    "final_model.add(Dropout(0.3))\n",
    "final_model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "final_model.add(Dropout(0.3))\n",
    "final_model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.1)))\n",
    "final_model.add(Dense(units=2, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 6.9793 - accuracy: 0.5256\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55183, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 5s 26ms/step - loss: 6.9793 - accuracy: 0.5256 - val_loss: 6.3433 - val_accuracy: 0.5518\n",
      "Epoch 2/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 6.3061 - accuracy: 0.5393\n",
      "Epoch 2: val_accuracy improved from 0.55183 to 0.56586, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 6.3025 - accuracy: 0.5399 - val_loss: 5.8231 - val_accuracy: 0.5659\n",
      "Epoch 3/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 5.7302 - accuracy: 0.5525\n",
      "Epoch 3: val_accuracy improved from 0.56586 to 0.57755, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 5.7302 - accuracy: 0.5525 - val_loss: 5.3427 - val_accuracy: 0.5776\n",
      "Epoch 4/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 5.2317 - accuracy: 0.5545\n",
      "Epoch 4: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 5.2287 - accuracy: 0.5551 - val_loss: 4.9063 - val_accuracy: 0.5690\n",
      "Epoch 5/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 4.7731 - accuracy: 0.5650\n",
      "Epoch 5: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 4.7706 - accuracy: 0.5654 - val_loss: 4.5017 - val_accuracy: 0.5682\n",
      "Epoch 6/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 4.3574 - accuracy: 0.5746\n",
      "Epoch 6: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 4.3547 - accuracy: 0.5745 - val_loss: 4.1256 - val_accuracy: 0.5705\n",
      "Epoch 7/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 3.9941 - accuracy: 0.5696\n",
      "Epoch 7: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 3.9941 - accuracy: 0.5696 - val_loss: 3.7820 - val_accuracy: 0.5713\n",
      "Epoch 8/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.6572 - accuracy: 0.5706\n",
      "Epoch 8: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 3.6560 - accuracy: 0.5708 - val_loss: 3.4671 - val_accuracy: 0.5752\n",
      "Epoch 9/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 3.3423 - accuracy: 0.5827\n",
      "Epoch 9: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 3.3405 - accuracy: 0.5830 - val_loss: 3.1796 - val_accuracy: 0.5760\n",
      "Epoch 10/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 3.0674 - accuracy: 0.5774\n",
      "Epoch 10: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 3.0665 - accuracy: 0.5777 - val_loss: 2.9184 - val_accuracy: 0.5698\n",
      "Epoch 11/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.8102 - accuracy: 0.5839\n",
      "Epoch 11: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 4s 23ms/step - loss: 2.8102 - accuracy: 0.5839 - val_loss: 2.6789 - val_accuracy: 0.5674\n",
      "Epoch 12/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 2.5760 - accuracy: 0.5881\n",
      "Epoch 12: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 2.5752 - accuracy: 0.5885 - val_loss: 2.4609 - val_accuracy: 0.5651\n",
      "Epoch 13/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 2.3606 - accuracy: 0.5915\n",
      "Epoch 13: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 2.3606 - accuracy: 0.5915 - val_loss: 2.2636 - val_accuracy: 0.5627\n",
      "Epoch 14/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 2.1659 - accuracy: 0.6012\n",
      "Epoch 14: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 2.1651 - accuracy: 0.6014 - val_loss: 2.0842 - val_accuracy: 0.5651\n",
      "Epoch 15/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 2.0034 - accuracy: 0.5968\n",
      "Epoch 15: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 2.0016 - accuracy: 0.5971 - val_loss: 1.9207 - val_accuracy: 0.5682\n",
      "Epoch 16/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 1.8401 - accuracy: 0.6055\n",
      "Epoch 16: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 5s 28ms/step - loss: 1.8394 - accuracy: 0.6050 - val_loss: 1.7749 - val_accuracy: 0.5713\n",
      "Epoch 17/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 1.7030 - accuracy: 0.6003\n",
      "Epoch 17: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 1.7025 - accuracy: 0.6004 - val_loss: 1.6435 - val_accuracy: 0.5729\n",
      "Epoch 18/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 1.5768 - accuracy: 0.6051\n",
      "Epoch 18: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 1.5761 - accuracy: 0.6053 - val_loss: 1.5249 - val_accuracy: 0.5721\n",
      "Epoch 19/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 1.4606 - accuracy: 0.6127\n",
      "Epoch 19: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 22ms/step - loss: 1.4595 - accuracy: 0.6140 - val_loss: 1.4188 - val_accuracy: 0.5666\n",
      "Epoch 20/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.3583 - accuracy: 0.6116\n",
      "Epoch 20: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 4s 23ms/step - loss: 1.3583 - accuracy: 0.6116 - val_loss: 1.3238 - val_accuracy: 0.5674\n",
      "Epoch 21/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 1.2686 - accuracy: 0.6081\n",
      "Epoch 21: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 1.2686 - accuracy: 0.6078 - val_loss: 1.2396 - val_accuracy: 0.5713\n",
      "Epoch 22/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 1.1909 - accuracy: 0.6113\n",
      "Epoch 22: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 1.1909 - accuracy: 0.6113 - val_loss: 1.1650 - val_accuracy: 0.5721\n",
      "Epoch 23/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 1.1184 - accuracy: 0.6154\n",
      "Epoch 23: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 1.1182 - accuracy: 0.6153 - val_loss: 1.0989 - val_accuracy: 0.5760\n",
      "Epoch 24/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 1.0558 - accuracy: 0.6109\n",
      "Epoch 24: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 22ms/step - loss: 1.0557 - accuracy: 0.6105 - val_loss: 1.0410 - val_accuracy: 0.5682\n",
      "Epoch 25/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 1.0023 - accuracy: 0.6155\n",
      "Epoch 25: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 1.0026 - accuracy: 0.6152 - val_loss: 0.9907 - val_accuracy: 0.5666\n",
      "Epoch 26/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.9525 - accuracy: 0.6190\n",
      "Epoch 26: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.9525 - accuracy: 0.6190 - val_loss: 0.9458 - val_accuracy: 0.5705\n",
      "Epoch 27/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.9124 - accuracy: 0.6164\n",
      "Epoch 27: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.9124 - accuracy: 0.6162 - val_loss: 0.9070 - val_accuracy: 0.5737\n",
      "Epoch 28/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.8757 - accuracy: 0.6178\n",
      "Epoch 28: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.8758 - accuracy: 0.6174 - val_loss: 0.8728 - val_accuracy: 0.5705\n",
      "Epoch 29/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.8432 - accuracy: 0.6119\n",
      "Epoch 29: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.8427 - accuracy: 0.6123 - val_loss: 0.8438 - val_accuracy: 0.5752\n",
      "Epoch 30/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.8151 - accuracy: 0.6169\n",
      "Epoch 30: val_accuracy did not improve from 0.57755\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.8151 - accuracy: 0.6169 - val_loss: 0.8184 - val_accuracy: 0.5776\n",
      "Epoch 31/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.7934 - accuracy: 0.6167\n",
      "Epoch 31: val_accuracy improved from 0.57755 to 0.58067, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.7934 - accuracy: 0.6167 - val_loss: 0.7968 - val_accuracy: 0.5807\n",
      "Epoch 32/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.7729 - accuracy: 0.6210\n",
      "Epoch 32: val_accuracy did not improve from 0.58067\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.7728 - accuracy: 0.6208 - val_loss: 0.7797 - val_accuracy: 0.5721\n",
      "Epoch 33/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.7528 - accuracy: 0.6256\n",
      "Epoch 33: val_accuracy did not improve from 0.58067\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.7528 - accuracy: 0.6256 - val_loss: 0.7640 - val_accuracy: 0.5768\n",
      "Epoch 34/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.7409 - accuracy: 0.6197\n",
      "Epoch 34: val_accuracy did not improve from 0.58067\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.7409 - accuracy: 0.6197 - val_loss: 0.7498 - val_accuracy: 0.5807\n",
      "Epoch 35/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.7274 - accuracy: 0.6222\n",
      "Epoch 35: val_accuracy improved from 0.58067 to 0.58223, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.7274 - accuracy: 0.6222 - val_loss: 0.7386 - val_accuracy: 0.5822\n",
      "Epoch 36/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.7162 - accuracy: 0.6234\n",
      "Epoch 36: val_accuracy improved from 0.58223 to 0.58535, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.7158 - accuracy: 0.6236 - val_loss: 0.7279 - val_accuracy: 0.5853\n",
      "Epoch 37/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.7087 - accuracy: 0.6211\n",
      "Epoch 37: val_accuracy did not improve from 0.58535\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.7087 - accuracy: 0.6209 - val_loss: 0.7212 - val_accuracy: 0.5822\n",
      "Epoch 38/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.6269\n",
      "Epoch 38: val_accuracy did not improve from 0.58535\n",
      "160/160 [==============================] - 3s 19ms/step - loss: 0.6996 - accuracy: 0.6269 - val_loss: 0.7158 - val_accuracy: 0.5814\n",
      "Epoch 39/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.6280\n",
      "Epoch 39: val_accuracy improved from 0.58535 to 0.58846, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6923 - accuracy: 0.6280 - val_loss: 0.7085 - val_accuracy: 0.5885\n",
      "Epoch 40/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6871 - accuracy: 0.6273\n",
      "Epoch 40: val_accuracy did not improve from 0.58846\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6873 - accuracy: 0.6267 - val_loss: 0.7045 - val_accuracy: 0.5830\n",
      "Epoch 41/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6845 - accuracy: 0.6222\n",
      "Epoch 41: val_accuracy did not improve from 0.58846\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6845 - accuracy: 0.6224 - val_loss: 0.6999 - val_accuracy: 0.5853\n",
      "Epoch 42/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6816 - accuracy: 0.6244\n",
      "Epoch 42: val_accuracy did not improve from 0.58846\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6811 - accuracy: 0.6254 - val_loss: 0.6964 - val_accuracy: 0.5853\n",
      "Epoch 43/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6769 - accuracy: 0.6269\n",
      "Epoch 43: val_accuracy did not improve from 0.58846\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6768 - accuracy: 0.6267 - val_loss: 0.6940 - val_accuracy: 0.5877\n",
      "Epoch 44/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6741 - accuracy: 0.6329\n",
      "Epoch 44: val_accuracy did not improve from 0.58846\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6746 - accuracy: 0.6323 - val_loss: 0.6935 - val_accuracy: 0.5838\n",
      "Epoch 45/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6726 - accuracy: 0.6271\n",
      "Epoch 45: val_accuracy did not improve from 0.58846\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6726 - accuracy: 0.6271 - val_loss: 0.6924 - val_accuracy: 0.5838\n",
      "Epoch 46/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6714 - accuracy: 0.6302\n",
      "Epoch 46: val_accuracy improved from 0.58846 to 0.58924, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6714 - accuracy: 0.6300 - val_loss: 0.6905 - val_accuracy: 0.5892\n",
      "Epoch 47/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6720 - accuracy: 0.6295\n",
      "Epoch 47: val_accuracy improved from 0.58924 to 0.59548, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6720 - accuracy: 0.6292 - val_loss: 0.6900 - val_accuracy: 0.5955\n",
      "Epoch 48/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6709 - accuracy: 0.6254\n",
      "Epoch 48: val_accuracy did not improve from 0.59548\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.6708 - accuracy: 0.6255 - val_loss: 0.6900 - val_accuracy: 0.5877\n",
      "Epoch 49/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6696 - accuracy: 0.6333\n",
      "Epoch 49: val_accuracy did not improve from 0.59548\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.6696 - accuracy: 0.6333 - val_loss: 0.6902 - val_accuracy: 0.5877\n",
      "Epoch 50/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6717 - accuracy: 0.6287\n",
      "Epoch 50: val_accuracy did not improve from 0.59548\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6720 - accuracy: 0.6288 - val_loss: 0.6903 - val_accuracy: 0.5877\n",
      "Epoch 51/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6691 - accuracy: 0.6355\n",
      "Epoch 51: val_accuracy did not improve from 0.59548\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6692 - accuracy: 0.6355 - val_loss: 0.6905 - val_accuracy: 0.5939\n",
      "Epoch 52/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6738 - accuracy: 0.6304\n",
      "Epoch 52: val_accuracy improved from 0.59548 to 0.59626, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.6738 - accuracy: 0.6304 - val_loss: 0.6920 - val_accuracy: 0.5963\n",
      "Epoch 53/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6735 - accuracy: 0.6294\n",
      "Epoch 53: val_accuracy did not improve from 0.59626\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6734 - accuracy: 0.6296 - val_loss: 0.6950 - val_accuracy: 0.5900\n",
      "Epoch 54/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6748 - accuracy: 0.6318\n",
      "Epoch 54: val_accuracy improved from 0.59626 to 0.59704, saving model to final_model.hdf5\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6751 - accuracy: 0.6313 - val_loss: 0.6955 - val_accuracy: 0.5970\n",
      "Epoch 55/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.6312\n",
      "Epoch 55: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6763 - accuracy: 0.6310 - val_loss: 0.6988 - val_accuracy: 0.5869\n",
      "Epoch 56/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6820 - accuracy: 0.6305\n",
      "Epoch 56: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.6823 - accuracy: 0.6296 - val_loss: 0.7025 - val_accuracy: 0.5885\n",
      "Epoch 57/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6838 - accuracy: 0.6306\n",
      "Epoch 57: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 3s 22ms/step - loss: 0.6838 - accuracy: 0.6306 - val_loss: 0.7052 - val_accuracy: 0.5869\n",
      "Epoch 58/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6869 - accuracy: 0.6300\n",
      "Epoch 58: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 3s 21ms/step - loss: 0.6869 - accuracy: 0.6300 - val_loss: 0.7083 - val_accuracy: 0.5955\n",
      "Epoch 59/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6913 - accuracy: 0.6346\n",
      "Epoch 59: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6910 - accuracy: 0.6345 - val_loss: 0.7147 - val_accuracy: 0.5869\n",
      "Epoch 60/200\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.6953 - accuracy: 0.6293\n",
      "Epoch 60: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 4s 22ms/step - loss: 0.6953 - accuracy: 0.6293 - val_loss: 0.7180 - val_accuracy: 0.5924\n",
      "Epoch 61/200\n",
      "158/160 [============================>.] - ETA: 0s - loss: 0.6986 - accuracy: 0.6328\n",
      "Epoch 61: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 3s 20ms/step - loss: 0.6992 - accuracy: 0.6317 - val_loss: 0.7227 - val_accuracy: 0.5885\n",
      "Epoch 62/200\n",
      "159/160 [============================>.] - ETA: 0s - loss: 0.7083 - accuracy: 0.6269\n",
      "Epoch 62: val_accuracy did not improve from 0.59704\n",
      "160/160 [==============================] - 3s 22ms/step - loss: 0.7081 - accuracy: 0.6275 - val_loss: 0.7292 - val_accuracy: 0.5916\n"
     ]
    }
   ],
   "source": [
    "filepath = 'final_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "callbacks = [checkpoint, early_stopping]\n",
    "\n",
    "final_model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.00005), metrics=['accuracy'])\n",
    "\n",
    "history = final_model.fit(X_train, train_labels,\n",
    "              epochs=200,\n",
    "              validation_data=(X_val, val_labels),\n",
    "              batch_size=64,\n",
    "              verbose=1,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "    '''\n",
    "    Input: Hisotry of Neural Network model\n",
    "    Prints: Training and validation accuracies\n",
    "    Output: None\n",
    "    '''\n",
    "    plt.plot(history.history['accuracy'], label='training data')\n",
    "    plt.plot(history.history['val_accuracy'], label='validation data')\n",
    "    plt.title('Categorical Accuracy for Text Classification')\n",
    "    plt.ylabel('Categorical Accuracy Value')\n",
    "    plt.xlabel('No. epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn_model(filepath, X, y):\n",
    "    '''\n",
    "    Input: hdf5 Model\n",
    "    Output: Model accuracy\n",
    "    '''\n",
    "    model = load_model(filepath)\n",
    "    yhat = model.predict(X)\n",
    "    y_pred = [0 if p[0] > 0.5 else 1 for p in yhat]\n",
    "    y_true = [0 if p[0] > 0.5 else 1 for p in y]\n",
    "    return accuracy_score(y_true, y_pred), precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc, val_prec = evaluate_nn_model(filepath, X_val, val_labels)\n",
    "print(f'Validation Accuracy: {round(val_acc, 4)}')\n",
    "print(f'Validation Precision: {round(val_prec, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_prec = evaluate_nn_model(filepath, X_test, test_labels)\n",
    "print(f'Test Accuracy: {round(test_acc, 4)}')\n",
    "print(f'Test Precision: {round(test_prec, 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm_with_labels(y_true, y_pred):\n",
    "    disp_labels = ['Fake', 'True']\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=disp_labels)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    disp.plot(ax=ax)\n",
    "    ax.grid(False)\n",
    "    disp.ax_.set_xticklabels(disp_labels)\n",
    "    return None\n",
    "\n",
    "\n",
    "final_model__ = load_model(filepath)\n",
    "y_pred = final_model__.predict(X_test)\n",
    "y_pred =  [0 if p[0] > 0.5 else 1 for p in y_pred]\n",
    "print_cm_with_labels(test_clean_df[\"output\"], y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
